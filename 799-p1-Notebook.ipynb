{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d58ef2",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa4899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "& was unexpected at this time.\n"
     ]
    }
   ],
   "source": [
    "# Load Virtual Environment\n",
    "!& \"c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Scripts\\Activate.ps1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bda5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9a73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = 'C:/Users/tbran/Python/repos/Semester 3 Repos/capstone/'\n",
    "data_path = project_path + 'data/'\n",
    "src_path = project_path + 'src/'\n",
    "model_path = project_path + 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc5532",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7983db",
   "metadata": {},
   "source": [
    "## Week 1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7713e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dataframe(df, datetime_cols=None, fillna=False):\n",
    "    \"\"\"\n",
    "    Cleans and optimizes a DataFrame:\n",
    "    - Converts object datetime columns to datetime64\n",
    "    - Converts object columns with repeated values to category\n",
    "    - Downcasts numeric columns to smallest safe type\n",
    "    - Optionally fills NaNs before downcasting\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to optimize\n",
    "        datetime_cols (list): List of column names to convert to datetime\n",
    "        fillna (bool): If True, fills NaNs before downcasting\n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Convert datetime columns\n",
    "    if datetime_cols:\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # 2. Convert object columns to category if appropriate\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df[col])\n",
    "        if num_unique / num_total < 0.5:  # heuristic: less than 50% unique\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    # 3. Downcast numeric columns\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "    float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n",
    "    \n",
    "    for col in int_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(0)\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in float_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
    "    print(f\"Reduced by {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_column_names(df, remove_accents=True):\n",
    "    \"\"\"\n",
    "    Cleans DataFrame column names:\n",
    "    - Strips whitespace\n",
    "    - Converts to lowercase\n",
    "    - Replaces spaces & special chars with underscores\n",
    "    - Removes duplicate underscores\n",
    "    - Optionally removes accents\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame whose columns to clean\n",
    "        remove_accents (bool): If True, strips accents from characters\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned column names\n",
    "    \"\"\"\n",
    "    def _clean(col):\n",
    "        col = col.strip().lower()\n",
    "        if remove_accents:\n",
    "            col = ''.join(\n",
    "                c for c in unicodedata.normalize('NFKD', col)\n",
    "                if not unicodedata.combining(c)\n",
    "            )\n",
    "        col = re.sub(r'[^0-9a-zA-Z]+', '_', col)  # replace non-alphanumeric with _\n",
    "        col = re.sub(r'_+', '_', col)             # collapse multiple underscores\n",
    "        col = col.strip('_')                      # remove leading/trailing underscores\n",
    "        return col\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.columns = [_clean(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def build_preprocessing_pipeline(df, target, \n",
    "                                  high_card_threshold=20, \n",
    "                                  scale_numeric=False):\n",
    "    \"\"\"\n",
    "    Builds a preprocessing pipeline for linear regression:\n",
    "    - One-hot encodes low-cardinality categorical columns\n",
    "    - Target encodes high-cardinality categorical columns\n",
    "    - Optionally scales numeric columns\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame (including target column)\n",
    "        target (str): Name of target column\n",
    "        high_card_threshold (int): Unique value cutoff for high-cardinality\n",
    "        scale_numeric (bool): Whether to scale numeric features\n",
    "        \n",
    "    Returns:\n",
    "        pipeline (ColumnTransformer): Preprocessing transformer\n",
    "        low_card_cols (list): Low-cardinality categorical columns\n",
    "        high_card_cols (list): High-cardinality categorical columns\n",
    "        num_cols (list): Numeric columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target])\n",
    "    \n",
    "    # Identify column types\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Split categorical into low/high cardinality\n",
    "    low_card_cols = [col for col in cat_cols if X[col].nunique() <= high_card_threshold]\n",
    "    high_card_cols = [col for col in cat_cols if X[col].nunique() > high_card_threshold]\n",
    "    \n",
    "    # Transformers\n",
    "    low_card_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    high_card_transformer = TargetEncoder()\n",
    "    num_transformer = StandardScaler() if scale_numeric else 'passthrough'\n",
    "    \n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('low_card', low_card_transformer, low_card_cols),\n",
    "            ('high_card', high_card_transformer, high_card_cols),\n",
    "            ('num', num_transformer, num_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor, low_card_cols, high_card_cols, num_cols\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df, features=None, vif_thresh=10.0):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) safely:\n",
    "    - Removes constant columns\n",
    "    - Removes perfectly collinear columns\n",
    "    - Returns sorted VIF table\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with numeric features\n",
    "        features (list): Optional list of features to check; defaults to all numeric\n",
    "        vif_thresh (float): Threshold for flagging high VIF\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: VIF table\n",
    "    \"\"\"\n",
    "    # Select numeric columns if features not provided\n",
    "    if features is None:\n",
    "        features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[features].copy()\n",
    "    \n",
    "    # 1. Drop constant columns\n",
    "    constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        print(f\"Dropping constant columns: {constant_cols}\")\n",
    "        X.drop(columns=constant_cols, inplace=True)\n",
    "    \n",
    "    # 2. Drop perfectly collinear columns\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    perfect_corr_cols = [col for col in upper.columns if any(upper[col] == 1.0)]\n",
    "    if perfect_corr_cols:\n",
    "        print(f\"Dropping perfectly collinear columns: {perfect_corr_cols}\")\n",
    "        X.drop(columns=perfect_corr_cols, inplace=True)\n",
    "    \n",
    "    # 3. Calculate VIF\n",
    "    X_const = X.assign(const=1)\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_const.values, i) for i in range(len(X.columns))]\n",
    "    })\n",
    "    \n",
    "    # 4. Sort by VIF\n",
    "    vif_data.sort_values(by=\"VIF\", ascending=False, inplace=True)\n",
    "    \n",
    "    # 5. Flag high VIF\n",
    "    vif_data[\"High_VIF\"] = vif_data[\"VIF\"] > vif_thresh\n",
    "    \n",
    "    return vif_data\n",
    "\n",
    "def fit_polynomial_regression(X, y, degree=2):\n",
    "    \"\"\"\n",
    "    Fits a polynomial regression model and returns the fitted model and transformed features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    return model, poly\n",
    "\n",
    "def add_interaction_terms(df, features):\n",
    "    \"\"\"\n",
    "    Adds pairwise interaction terms between given features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    interaction_array = poly.fit_transform(df[features])\n",
    "    interaction_df = pd.DataFrame(interaction_array, columns=poly.get_feature_names_out(features))\n",
    "    return pd.concat([df.reset_index(drop=True), interaction_df], axis=1)\n",
    "\n",
    "def preprocess_features(df, categorical_cols, numeric_cols):\n",
    "    \"\"\"\n",
    "    Returns a ColumnTransformer that one-hot encodes categorical columns\n",
    "    and passes numeric columns through unchanged.\n",
    "    \"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "            ('num', 'passthrough', numeric_cols)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def regression_summary(X, y):\n",
    "    \"\"\"\n",
    "    Fits an OLS regression model using statsmodels and prints the summary.\n",
    "    \"\"\"\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32311e0",
   "metadata": {},
   "source": [
    "# Week 1 Notebook – Linear Regression 1\n",
    "Each week, you will apply the concepts of that week to your Integrated Capstone Project’s dataset. In preparation for Milestone One, create a Jupyter Notebook (similar to in Module B, semester two) that illustrates these lessons. There are no specific questions to answer in your Jupyter Notebook files in this course; your general goal is to analyze your data, using the methods you have learned about in this course and in this program, and draw interesting conclusions. \n",
    "\n",
    "For Week 1, include concepts such as linear regression with polynomial terms, interaction terms, multicollinearity, variance inflation factor and regression, and categorical and continuous features. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209b52c",
   "metadata": {},
   "source": [
    "## Mendeley Delay Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438b27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "df = pd.read_csv(data_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1404dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n",
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n"
     ]
    }
   ],
   "source": [
    "df = optimize_dataframe(\n",
    "    df,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df = clean_column_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2046a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column categories\n",
    "\n",
    "id_cols = ['originairportid', 'destairportid', ]\n",
    "cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "date_cols = ['scheduleddepartdatetime', ]\n",
    "target_cols = ['depdelay','arrdelay',]\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + cat_cols + date_cols + target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d437e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['uniquecarrier']\n",
      "High-cardinality categorical: ['origin', 'dest', 'tailnum', 'origincityname', 'originstate']\n",
      "Numeric columns: ['marketshareorigin', 'marketsharedest', 'hhiorigin', 'hhidest', 'nonhubairportorigin', 'smallhubairportorigin', 'mediumhubairportorigin', 'largehubairportorigin', 'nonhubairportdest', 'smallhubairportdest', 'mediumhubairportdest', 'largehubairportdest', 'nonhubairlineorigin', 'smallhubairlineorigin', 'mediumhubairlineorigin', 'largehubairlineorigin', 'nonhubairlinedest', 'smallhubairlinedest', 'mediumhubairlinedest', 'largehubairlinedest', 'year', 'month', 'dayofmonth', 'dayofweek', 'scheduledhour', 'capacity', 'loadfactor', 'numflights', 'distance', 'monopolyroute', 'temperature', 'temp_ninfty_n10', 'temp_n10_0', 'temp_0_10', 'temp_10_20', 'temp_20_30', 'temp_30_40', 'temp_40_infty', 'windspeed', 'windspeedsquare', 'windgustdummy', 'windgustspeed', 'raindummy', 'raintracedummy', 'snowdummy', 'snowtracedummy', 'originmetropop', 'originmetrogdppercapita', 'destmetropop', 'destmetrogdppercapita']\n",
      "R^2 score: 0.043766421823172585\n"
     ]
    }
   ],
   "source": [
    "# drop leakage columns for linear regression\n",
    "df_lin = df.drop(columns=['arrdelay'] + id_cols + date_cols).copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    df_lin, \n",
    "    target='depdelay', \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = df_lin.drop(columns=['depdelay'])\n",
    "y = df_lin['depdelay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f91cb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    feature           VIF  High_VIF\n",
      "13      largehubairportdest           inf      True\n",
      "12     mediumhubairportdest           inf      True\n",
      "10        nonhubairportdest           inf      True\n",
      "11      smallhubairportdest           inf      True\n",
      "7     smallhubairportorigin           inf      True\n",
      "6       nonhubairportorigin           inf      True\n",
      "18        nonhubairlinedest           inf      True\n",
      "19      smallhubairlinedest           inf      True\n",
      "16   mediumhubairlineorigin           inf      True\n",
      "17    largehubairlineorigin           inf      True\n",
      "20     mediumhubairlinedest           inf      True\n",
      "21      largehubairlinedest           inf      True\n",
      "15    smallhubairlineorigin           inf      True\n",
      "14      nonhubairlineorigin           inf      True\n",
      "8    mediumhubairportorigin  9.007199e+15      True\n",
      "9     largehubairportorigin  9.007199e+15      True\n",
      "39               temp_20_30  3.112541e+02      True\n",
      "38               temp_10_20  2.888918e+02      True\n",
      "37                temp_0_10  1.916812e+02      True\n",
      "40               temp_30_40  1.151481e+02      True\n",
      "36               temp_n10_0  7.219096e+01      True\n",
      "34              temperature  1.429181e+01      True\n",
      "35          temp_ninfty_n10  1.244962e+01      True\n",
      "43          windspeedsquare  9.783338e+00     False\n",
      "42                windspeed  8.658978e+00     False\n",
      "1                  arrdelay  8.096892e+00     False\n",
      "0                  depdelay  8.012084e+00     False\n",
      "41            temp_40_infty  5.988440e+00     False\n",
      "3           marketsharedest  3.643296e+00     False\n",
      "2         marketshareorigin  3.596286e+00     False\n",
      "22                     year  3.081408e+00     False\n",
      "45            windgustspeed  2.712213e+00     False\n",
      "31               numflights  2.603665e+00     False\n",
      "5                   hhidest  2.493102e+00     False\n",
      "4                 hhiorigin  2.430416e+00     False\n",
      "44            windgustdummy  2.181984e+00     False\n",
      "33            monopolyroute  1.598653e+00     False\n",
      "32                 distance  1.564135e+00     False\n",
      "51  originmetrogdppercapita  1.519503e+00     False\n",
      "30               loadfactor  1.452184e+00     False\n",
      "53    destmetrogdppercapita  1.450741e+00     False\n",
      "29                 capacity  1.332612e+00     False\n",
      "50           originmetropop  1.326248e+00     False\n",
      "52             destmetropop  1.326226e+00     False\n",
      "49           snowtracedummy  1.181472e+00     False\n",
      "27          originairportid  1.173991e+00     False\n",
      "25                dayofweek  1.165972e+00     False\n",
      "28            destairportid  1.160076e+00     False\n",
      "48                snowdummy  1.105665e+00     False\n",
      "26            scheduledhour  1.084384e+00     False\n",
      "46                raindummy  1.061760e+00     False\n",
      "23                    month  1.048324e+00     False\n",
      "47           raintracedummy  1.024877e+00     False\n",
      "24               dayofmonth  1.000590e+00     False\n"
     ]
    }
   ],
   "source": [
    "vif_table = calculate_vif(df)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f78e22",
   "metadata": {},
   "source": [
    "## USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96001c2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Import' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSyntaxError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\IPython\\core\\completer.py:1319\u001b[39m, in \u001b[36mCompleter._evaluate_expr\u001b[39m\u001b[34m(self, expr)\u001b[39m\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     obj = \u001b[43mguarded_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEvaluationContext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglobal_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m            \u001b[49m\u001b[43mauto_import\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_import\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpolicy_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1329\u001b[39m     done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\IPython\\core\\guarded_eval.py:422\u001b[39m, in \u001b[36mguarded_eval\u001b[39m\u001b[34m(code, context)\u001b[39m\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28meval\u001b[39m(code, context.globals, context.locals)\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m expression = \u001b[43mast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m eval_node(expression, context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ast.py:50\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(source, filename, mode, type_comments, feature_version)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Else it should be an int giving the minor version for 3.x.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcompile\u001b[39m(source, filename, mode, flags,\n\u001b[32m     51\u001b[39m                _feature_version=feature_version)\n",
      "\u001b[31mSyntaxError\u001b[39m: invalid syntax (<unknown>, line 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\IPython\\core\\completer.py:3516\u001b[39m, in \u001b[36mIPCompleter._complete\u001b[39m\u001b[34m(self, cursor_line, cursor_pos, line_buffer, text, full_text)\u001b[39m\n\u001b[32m   3512\u001b[39m     result = _convert_matcher_v1_result_to_v2_no_no(\n\u001b[32m   3513\u001b[39m         matcher(text), \u001b[38;5;28mtype\u001b[39m=_UNKNOWN_TYPE\n\u001b[32m   3514\u001b[39m     )\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_matcher_v2(matcher):\n\u001b[32m-> \u001b[39m\u001b[32m3516\u001b[39m     result = \u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3518\u001b[39m     api_version = _get_matcher_api_version(matcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\IPython\\core\\completer.py:2625\u001b[39m, in \u001b[36mIPCompleter.python_matcher\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m completion_type == \u001b[38;5;28mself\u001b[39m._CompletionContextType.ATTRIBUTE:\n\u001b[32m   2624\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2625\u001b[39m         matches, fragment = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attr_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2626\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m text.endswith(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.omit__names:\n\u001b[32m   2627\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.omit__names == \u001b[32m1\u001b[39m:\n\u001b[32m   2628\u001b[39m                 \u001b[38;5;66;03m# true if txt is _not_ a __ name, false otherwise:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\IPython\\core\\completer.py:1234\u001b[39m, in \u001b[36mCompleter._attr_matches\u001b[39m\u001b[34m(self, text, include_prefix)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m tokenize.TokenError:\n\u001b[32m   1232\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1234\u001b[39m obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m not_found:\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [], \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\IPython\\core\\completer.py:1338\u001b[39m, in \u001b[36mCompleter._evaluate_expr\u001b[39m\u001b[34m(self, expr)\u001b[39m\n\u001b[32m   1329\u001b[39m     done = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSyntaxError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m   1331\u001b[39m     \u001b[38;5;66;03m# TypeError can show up with something like `+ d`\u001b[39;00m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;66;03m# where `d` is a dictionary.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1336\u001b[39m     \u001b[38;5;66;03m# where parenthesis is not closed.\u001b[39;00m\n\u001b[32m   1337\u001b[39m     \u001b[38;5;66;03m# TODO: make this faster by reusing parts of the computation?\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m     expr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_trim_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\IPython\\core\\completer.py:1306\u001b[39m, in \u001b[36mCompleter._trim_expr\u001b[39m\u001b[34m(self, code)\u001b[39m\n\u001b[32m   1304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res.body) != \u001b[32m1\u001b[39m:\n\u001b[32m   1305\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1306\u001b[39m expr = \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr, ast.Tuple) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m code[-\u001b[32m1\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1308\u001b[39m     \u001b[38;5;66;03m# we skip implicit tuple, like when trimming `fun(a,b`<completion>\u001b[39;00m\n\u001b[32m   1309\u001b[39m     \u001b[38;5;66;03m# as `a,b` would be a tuple, and we actually expect to get only `b`\u001b[39;00m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Import' object has no attribute 'value'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_25484\\508701689.py:5: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_25484\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_25484\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_25484\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_25484\\508701689.py:5: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f18530e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 3779.75 MB\n",
      "Memory usage after optimization: 535.69 MB\n",
      "Reduced by 85.8%\n"
     ]
    }
   ],
   "source": [
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d649ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4434d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n",
      "R^2 score: 0.06941058350132379\n"
     ]
    }
   ],
   "source": [
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "usdot_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "usdot_model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", usdot_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d909dbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping constant columns: ['year', 'flights']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vif_table = \u001b[43mcalculate_vif\u001b[49m\u001b[43m(\u001b[49m\u001b[43musdot_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(vif_table)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36mcalculate_vif\u001b[39m\u001b[34m(df, features, vif_thresh)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# 3. Calculate VIF\u001b[39;00m\n\u001b[32m    178\u001b[39m X_const = X.assign(const=\u001b[32m1\u001b[39m)\n\u001b[32m    179\u001b[39m vif_data = pd.DataFrame({\n\u001b[32m    180\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m\"\u001b[39m: X.columns,\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mVIF\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43m[\u001b[49m\u001b[43mvariance_inflation_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_const\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    182\u001b[39m })\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# 4. Sort by VIF\u001b[39;00m\n\u001b[32m    185\u001b[39m vif_data.sort_values(by=\u001b[33m\"\u001b[39m\u001b[33mVIF\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# 3. Calculate VIF\u001b[39;00m\n\u001b[32m    178\u001b[39m X_const = X.assign(const=\u001b[32m1\u001b[39m)\n\u001b[32m    179\u001b[39m vif_data = pd.DataFrame({\n\u001b[32m    180\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m\"\u001b[39m: X.columns,\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mVIF\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[43mvariance_inflation_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_const\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X.columns))]\n\u001b[32m    182\u001b[39m })\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# 4. Sort by VIF\u001b[39;00m\n\u001b[32m    185\u001b[39m vif_data.sort_values(by=\u001b[33m\"\u001b[39m\u001b[33mVIF\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:196\u001b[39m, in \u001b[36mvariance_inflation_factor\u001b[39m\u001b[34m(exog, exog_idx)\u001b[39m\n\u001b[32m    194\u001b[39m mask = np.arange(k_vars) != exog_idx\n\u001b[32m    195\u001b[39m x_noti = exog[:, mask]\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m r_squared_i = \u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_noti\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.rsquared\n\u001b[32m    197\u001b[39m vif = \u001b[32m1.\u001b[39m / (\u001b[32m1.\u001b[39m - r_squared_i)\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vif\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:333\u001b[39m, in \u001b[36mRegressionModel.fit\u001b[39m\u001b[34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mpinv\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpinv_wexog\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnormalized_cov_params\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    331\u001b[39m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m         \u001b[38;5;28mself\u001b[39m.pinv_wexog, singular_values = \u001b[43mpinv_extended\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28mself\u001b[39m.normalized_cov_params = np.dot(\n\u001b[32m    335\u001b[39m             \u001b[38;5;28mself\u001b[39m.pinv_wexog, np.transpose(\u001b[38;5;28mself\u001b[39m.pinv_wexog))\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# Cache these singular values for use later.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\tools\\tools.py:264\u001b[39m, in \u001b[36mpinv_extended\u001b[39m\u001b[34m(x, rcond)\u001b[39m\n\u001b[32m    262\u001b[39m x = np.asarray(x)\n\u001b[32m    263\u001b[39m x = x.conjugate()\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m u, s, vt = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m s_orig = np.copy(s)\n\u001b[32m    266\u001b[39m m = u.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:1862\u001b[39m, in \u001b[36msvd\u001b[39m\u001b[34m(a, full_matrices, compute_uv, hermitian)\u001b[39m\n\u001b[32m   1858\u001b[39m signature = \u001b[33m'\u001b[39m\u001b[33mD->DdD\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33md->ddd\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1859\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call=_raise_linalgerror_svd_nonconvergence,\n\u001b[32m   1860\u001b[39m               invalid=\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m, over=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1861\u001b[39m               under=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1862\u001b[39m     u, s, vh = \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1863\u001b[39m u = u.astype(result_t, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1864\u001b[39m s = s.astype(_realType(result_t), copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "vif_table = calculate_vif(usdot_df)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9f2ac",
   "metadata": {},
   "source": [
    "# Week 2 Notebook - Linear Regression 2\n",
    "\n",
    "For Week 2, include concepts such as linear regression with lasso, ridge, and elastic net regression. This homework will be submitted for peer review and feedback in Week 3 in the assignment titled 3.4 Peer Review: Week 2 Jupyter Notebook. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8fd697",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4b684",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ada4666c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n",
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n",
      "Low-cardinality categorical: ['uniquecarrier']\n",
      "High-cardinality categorical: ['origin', 'dest', 'tailnum', 'origincityname', 'originstate']\n",
      "Numeric columns: ['marketshareorigin', 'marketsharedest', 'hhiorigin', 'hhidest', 'nonhubairportorigin', 'smallhubairportorigin', 'mediumhubairportorigin', 'largehubairportorigin', 'nonhubairportdest', 'smallhubairportdest', 'mediumhubairportdest', 'largehubairportdest', 'nonhubairlineorigin', 'smallhubairlineorigin', 'mediumhubairlineorigin', 'largehubairlineorigin', 'nonhubairlinedest', 'smallhubairlinedest', 'mediumhubairlinedest', 'largehubairlinedest', 'year', 'month', 'dayofmonth', 'dayofweek', 'scheduledhour', 'capacity', 'loadfactor', 'numflights', 'distance', 'monopolyroute', 'temperature', 'temp_ninfty_n10', 'temp_n10_0', 'temp_0_10', 'temp_10_20', 'temp_20_30', 'temp_30_40', 'temp_40_infty', 'windspeed', 'windspeedsquare', 'windgustdummy', 'windgustspeed', 'raindummy', 'raintracedummy', 'snowdummy', 'snowtracedummy', 'originmetropop', 'originmetrogdppercapita', 'destmetropop', 'destmetrogdppercapita']\n"
     ]
    }
   ],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "df = pd.read_csv(data_path + file_name)\n",
    "\n",
    "df = optimize_dataframe(\n",
    "    df,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df = clean_column_names(df)\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['originairportid', 'destairportid', ]\n",
    "cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "date_cols = ['scheduleddepartdatetime', ]\n",
    "target_cols = ['depdelay','arrdelay',]\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "df_lin = df.drop(columns=['arrdelay'] + id_cols + date_cols).copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    df_lin, \n",
    "    target='depdelay', \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = df_lin.drop(columns=['depdelay'])\n",
    "y = df_lin['depdelay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a18f",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0be88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.0337074407107556\n"
     ]
    }
   ],
   "source": [
    "# Default Lasso \n",
    "\n",
    "lasso = Lasso(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lasso)\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491616f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso with tuned alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6d279",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge with default alpha\n",
    "\n",
    "ridge = Ridge(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ridge)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default alpha\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha):\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19aaf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge with tune alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c99e1",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace regressor with ElasticNet\n",
    "elastic = ElasticNet(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', elastic)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default parameters\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha, l1_ratio):\", model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a67925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Tune alpha and l1_ratio with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'regressor__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35472fe2",
   "metadata": {},
   "source": [
    "# Week 3 Notebook - Linear Regression 3\n",
    "\n",
    "For Week 3, include concepts such as linear regression with forward and backward selection, PCR, and PLSR. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b99d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd613b8",
   "metadata": {},
   "source": [
    "# Week 4 Notebook - Logistic Regression and Feature Scaling\n",
    "\n",
    "For Week 4, include concepts such as logistic regression and feature scaling. This homework should be submitted for peer review in the assignment titled 4.3 Peer Review: Week 4 Jupyter Notebook. Complete and submit your Jupyter Notebook homework by 11:59pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57cfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
